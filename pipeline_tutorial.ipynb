{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MetaBeeAI LLM pipeline tutorial\n",
    "\n",
    "Walk through the steps of the MetaBeeAI LLM pipeline.\n",
    "\n",
    "1. Split the PDF into overlapping 2-page PDFs\n",
    "2. Process the PDFs using Vision Agentic Document Analysis\n",
    "3. Extract the text from the PDFs (semi-structured JSON format)\n",
    "4. Extract the structured data from the text (structured JSON and csv format)\n",
    "\n",
    "Starting with a directory of PDFs, each within a numbered 3-digit subfolder, e.g.:\n",
    "\n",
    "```\n",
    "data/papers/\n",
    "├── 001/\n",
    "│   ├── main.pdf\n",
    "└── 002/\n",
    "    ├── main.pdf\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: create virtual environment\n",
    "\n",
    "```\n",
    "python -m venv metabeeai_llm\n",
    "source metabeeai_llm/bin/activate\n",
    "```\n",
    "\n",
    "Step 2: install dependencies\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Step 3: create .env file with your API keys\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "LANDING_AI_API_KEY=your_landing_ai_api_key\n",
    "ANTHROPIC_API_KEY=your_anthropic_api_key\n",
    "```\n",
    "\n",
    "Step 4: install the package\n",
    "\n",
    "```\n",
    "pip install -e . # in development mode\n",
    "```\n",
    "\n",
    "or:\n",
    "\n",
    "```\n",
    "pip install .\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split the PDF into overlapping 2-page PDFs using `split_pdf.py`\n",
    "\n",
    "The script will:\n",
    "- Look for subfolders in the specified directory\n",
    "- For each subfolder, look for a PDF named {subfolder}_main.pdf\n",
    "- Create a \"pages\" subdirectory if it doesn't exist\n",
    "- Split the PDF into overlapping 2-page PDFs named main_p01-02.pdf, main_p02-03.pdf, etc.\n",
    "\n",
    "Note that this script requires exactly one argument - the directory containing your paper subfolders. \n",
    "\n",
    "The directory should have this structure:\n",
    "\n",
    "papers_directory/\n",
    "    001/\n",
    "        001_main.pdf\n",
    "    002/\n",
    "        002_main.pdf\n",
    "    ...\n",
    "\n",
    "Basic usage (requires directory argument)\n",
    "```\n",
    "python -m process_pdfs.split_pdf data/papers\n",
    "```\n",
    "\n",
    "Or with full path\n",
    "```\n",
    "python -m process_pdfs.split_pdf /path/to/papers/directory\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_pdfs.split_pdf import split_pdfs\n",
    "\n",
    "# Basic usage\n",
    "split_pdfs(\"data/papers\")\n",
    "\n",
    "# Or with full path\n",
    "split_pdfs(\"/path/to/papers/directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Vision Agentic Document Analysis\n",
    "\n",
    "Vision Agentic Document Analysis uses computer vision to extract text from the PDFs. It outputs a JSON file for each PDF, with chunks of text (or image descriptions) labeled with unique IDs and chunk types (e.g., \"text\", \"figure\", \"header\", etc.).\n",
    "\n",
    "Parameters:\n",
    "--dir / papers_dir: Directory containing paper folders (default: \"data/papers\")\n",
    "--start / start_folder: Optional folder number to start processing from (e.g., \"059\")\n",
    "\n",
    "The script will:\n",
    "- Look for PDF files in the \"pages\" subdirectory of each paper folder\n",
    "- Process each PDF using the Landing AI Vision Agentic Document Analysis API\n",
    "- Save the results as JSON files alongside the PDFs\n",
    "- Create a timestamped log file in the papers directory\n",
    "\n",
    "Note: Make sure you have:\n",
    "- Set up your .env file with LANDING_AI_API_KEY\n",
    "\n",
    "The required directory structure:\n",
    "papers_directory/\n",
    "    001/\n",
    "        pages/\n",
    "            main_p01-02.pdf\n",
    "            main_p02-03.pdf\n",
    "            ...\n",
    "    002/\n",
    "        pages/\n",
    "            ...\n",
    "\n",
    "Basic usage (uses default data/papers directory)\n",
    "```\n",
    "python -m process_pdfs.va_process_papers\n",
    "```\n",
    "With specific directory\n",
    "```\n",
    "python -m process_pdfs.va_process_papers --dir data/papers\n",
    "```\n",
    "Start from a specific folder number\n",
    "```\n",
    "python -m process_pdfs.va_process_papers --dir data/papers --start 059\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_pdfs.va_process_papers import process_papers\n",
    "\n",
    "# Basic usage (uses default \"../data/papers\" directory)\n",
    "process_papers()\n",
    "\n",
    "# With specific directory\n",
    "process_papers(papers_dir=\"data/papers\")\n",
    "\n",
    "# Start from a specific folder number\n",
    "process_papers(papers_dir=\"data/papers\", start_folder=\"059\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run first LLM pipeline with `llm_pipeline.py` \n",
    "\n",
    "(using `json_multistage_qa.py`)\n",
    "\n",
    "This pipeline extracts inital structured data from the text chunks. It aims to get information about the bees and pesticides, as well as any secondary stressors. It compiles a list of endpoints found in the text chunks and attempts to extract sample size, means (central tendency), and standard deviations (if available). The pipeline also extracts text summaries of the discussion, including the impact of the paper's findings, recommendations for future research, and any limitations of the study. \n",
    "\n",
    "First, the output JSON files from Vision Agentic Document Analysis are merged into a single JSON file.\n",
    "\n",
    "Then, the LLM pipeline is run to extract the literature answers.\n",
    "\n",
    "It uses questions.yaml to determine the questions and formats the output JSON file.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "--dir / base_dir: Directory containing paper folders (default: \"../data/papers\")\n",
    "\n",
    "--start / start_paper: First paper number to process (default: 1)\n",
    "\n",
    "--end / end_paper: Last paper number to process (default: 999)\n",
    "\n",
    "--overwrite / overwrite_merged: Flag to overwrite existing merged.json files (default: False)\n",
    "\n",
    "The script will process the papers in the specified range and create or update merged.json files in each paper's directory.\n",
    "\n",
    "Basic usage (uses defaults)\n",
    "```\n",
    "python -m metabeeai_llm.llm_pipeline\n",
    "```\n",
    "\n",
    "With specific options\n",
    "\n",
    "```\n",
    "python -m metabeeai_llm.llm_pipeline \\\n",
    "    --dir data/papers \\\n",
    "    --start 1 \\\n",
    "    --end 10 \\\n",
    "    --overwrite\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metabeeai_llm.llm_pipeline import process_papers\n",
    "\n",
    "# Basic usage (uses defaults)\n",
    "process_papers(\n",
    "    base_dir=\"../data/papers\",\n",
    "    start_paper=1,\n",
    "    end_paper=999,\n",
    "    overwrite_merged=False\n",
    ")\n",
    "\n",
    "# With specific options\n",
    "process_papers(\n",
    "    base_dir=\"data/papers\",\n",
    "    start_paper=1,\n",
    "    end_paper=10,\n",
    "    overwrite_merged=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick check of chunk IDs to make sure they are unique\n",
    "\n",
    "The script will:\n",
    "- Check all JSON files (except merged.json) in each pages/ subfolder\n",
    "- Look for duplicate chunk_ids\n",
    "- Print any duplicates found\n",
    "- Save a detailed log file with results to either:\n",
    "    - data/logs/chunk_id_check_[timestamp].json (for relative paths)\n",
    "    - [papers_dir]/logs/chunk_id_check_[timestamp].json (for absolute paths)\n",
    "The log file will contain a summary of how many subfolders were checked and how many had duplicates, along with detailed information about any duplicates found.\n",
    "\n",
    "Basic usage (uses default data/papers directory)\n",
    "```\n",
    "python -m metabeeai_llm.unique_chunk_id\n",
    "```\n",
    "\n",
    "Specify a custom directory\n",
    "``` \n",
    "python -m metabeeai_llm.unique_chunk_id --dir path/to/papers\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metabeeai_llm.unique_chunk_id import check_chunk_ids_in_pages_dir\n",
    "\n",
    "# Basic usage (uses default data/papers directory)\n",
    "check_chunk_ids_in_pages_dir()\n",
    "\n",
    "# Specify a custom directory\n",
    "check_chunk_ids_in_pages_dir(papers_dir=\"path/to/papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run second LLM pipeline with `process_llm_output.py`\n",
    "\n",
    "This pipeline extracts structured data from the JSON files. It uses the `schema_config.yaml` file to determine which chunks to extract and how to structure the data.\n",
    "\n",
    "It has two options: \n",
    "\n",
    "1. Extract pesticides and bee data\n",
    "2. Extract endpoints, means, standard deviations, and sample sizes\n",
    "\n",
    "\n",
    "Basic usage - process both pesticides and endpoints\n",
    "```\n",
    "python -m metabeeai_llm.process_llm_output --start 1 --end 10\n",
    "```\n",
    "Process only pesticides\n",
    "```\n",
    "python -m metabeeai_llm.process_llm_output --start 1 --end 10 --pesticides\n",
    "```\n",
    "Process only endpoints\n",
    "```\n",
    "python -m metabeeai_llm.process_llm_output --start 1 --end 10 --endpoints\n",
    "```\n",
    "Process both explicitly\n",
    "```\n",
    "python -m metabeeai_llm.process_llm_output --start 1 --end 10 --all\n",
    "```\n",
    "With custom config file\n",
    "```\n",
    "python -m metabeeai_llm.process_llm_output --start 1 --end 10 --config path/to/schema_config.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metabeeai_llm.process_llm_output import process_papers, save_data, flatten_pesticide_data\n",
    "\n",
    "# Process papers\n",
    "pesticide_results, endpoint_results = process_papers(\n",
    "    start_folder=1,\n",
    "    end_folder=10,\n",
    "    config_file=\"../schema_config.yaml\",  # optional\n",
    "    process_pesticides=True,              # optional\n",
    "    process_endpoints=True                # optional\n",
    ")\n",
    "\n",
    "# Save the results\n",
    "if pesticide_results:\n",
    "    save_data(pesticide_results, \"pesticides\", flatten_func=flatten_pesticide_data)\n",
    "if endpoint_results:\n",
    "    save_data(endpoint_results, \"endpoints\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
