# MetaBeeAI LLM Pipeline - Parallel Processing & Hybrid Models

## üöÄ Overview

The MetaBeeAI LLM pipeline now features **parallel processing** and **hybrid model architecture** for optimal performance and cost-effectiveness.

## üéØ Key Features

### 1. **Hybrid Model Architecture**
- **Fast Model (GPT-4o-mini)**: Used for relevance scoring to quickly filter chunks
- **High-Quality Model (GPT-4o)**: Used for answer generation to ensure quality
- **Configurable**: Easy to switch between different model combinations

### 2. **Parallel Processing**
- **Relevance Scoring**: Process up to 20 chunks simultaneously
- **Answer Generation**: Process up to 5 chunks simultaneously  
- **Batch Processing**: Intelligent batching to avoid API rate limits
- **Configurable Batch Sizes**: Adjust based on your API limits and needs

### 3. **Performance Optimization**
- **Concurrent API Calls**: Maximize throughput while respecting rate limits
- **Smart Batching**: Process chunks in optimal batch sizes
- **Rate Limiting Protection**: Built-in delays and request management

## ‚öôÔ∏è Configuration

### Model Selection

Edit `pipeline_config.py` to choose your preferred configuration:

```python
# Option 1: Fast and Cost-Effective (Recommended for high-volume)
CURRENT_CONFIG = FAST_CONFIG

# Option 2: Balanced Performance (Recommended for most use cases)  
CURRENT_CONFIG = BALANCED_CONFIG

# Option 3: High Quality (Recommended for critical analysis)
CURRENT_CONFIG = QUALITY_CONFIG
```

### Parallel Processing Settings

```python
PARALLEL_CONFIG = {
    'relevance_batch_size': 20,    # Chunks processed in parallel for relevance
    'answer_batch_size': 5,        # Chunks processed in parallel for answers
    'max_concurrent_requests': 25, # Maximum concurrent API requests
    'batch_delay': 0.1,            # Delay between batches (seconds)
}
```

## üìä Performance Benefits

### Speed Improvements
- **Relevance Scoring**: 10-20x faster with parallel processing
- **Answer Generation**: 3-5x faster with parallel processing
- **Overall Pipeline**: 5-15x faster depending on chunk count

### Cost Optimization
- **Hybrid Approach**: Use expensive models only where needed
- **Batch Processing**: Reduce API call overhead
- **Smart Filtering**: Process fewer chunks with high-quality models

## üîß Usage Examples

### Basic Usage
```python
from json_multistage_qa import ask_json

# Process a single question
result = await ask_json(
    question="What species of bee(s) were tested?",
    json_path="papers/001/pages/merged_v2.json"
)
```

### Testing Parallel Processing
```bash
# Test the parallel processing capabilities
python test_parallel_processing.py

# View current configuration
python pipeline_config.py
```

## üìà Expected Performance

### For a typical paper (150-200 chunks):

| Configuration | Sequential Time | Parallel Time | Speedup |
|---------------|----------------|---------------|---------|
| **Fast** (GPT-4o-mini) | ~10-15 minutes | ~2-3 minutes | **5-7x** |
| **Balanced** (Hybrid) | ~15-20 minutes | ~3-5 minutes | **4-6x** |
| **Quality** (GPT-4o) | ~20-30 minutes | ~5-8 minutes | **3-5x** |

## üö® Important Notes

### Rate Limiting
- The pipeline automatically manages API rate limits
- Batch sizes are configurable to match your API tier
- Built-in delays prevent overwhelming the API

### Memory Usage
- Parallel processing increases memory usage
- Monitor memory usage for very large papers
- Consider reducing batch sizes if memory becomes an issue

### API Costs
- **GPT-4o-mini**: ~$0.00015 per 1K tokens
- **GPT-4o**: ~$0.005 per 1K tokens
- Hybrid approach typically reduces costs by 40-60%

## üîç Troubleshooting

### Common Issues

1. **Rate Limiting Errors**
   - Reduce `max_concurrent_requests`
   - Increase `batch_delay`
   - Check your API tier limits

2. **Memory Issues**
   - Reduce batch sizes
   - Process papers one at a time
   - Monitor system resources

3. **Model Availability**
   - Ensure your API key has access to both models
   - Check model availability in your region
   - Fall back to single-model configuration if needed

### Debug Mode
```python
# Enable detailed logging
PERFORMANCE_CONFIG = {
    'enable_detailed_logging': True
}
```

## üìö Advanced Configuration

### Custom Model Combinations
```python
CUSTOM_CONFIG = {
    'relevance_model': 'openai/gpt-3.5-turbo',  # Custom relevance model
    'answer_model': 'openai/gpt-4o',            # Custom answer model
    'description': 'Custom configuration'
}
```

### Performance Tuning
```python
PERFORMANCE_CONFIG = {
    'enable_parallel_processing': True,   # Enable/disable parallel processing
    'enable_batch_processing': True,      # Enable/disable batch processing
    'enable_progress_bars': True,         # Show progress bars
    'enable_detailed_logging': False,     # Detailed logging
}
```

## ü§ù Contributing

To optimize the pipeline further:
1. Test different batch sizes for your use case
2. Experiment with model combinations
3. Monitor performance metrics
4. Share your findings and optimizations

## üìû Support

For questions or issues:
1. Check the configuration settings
2. Review the troubleshooting section
3. Test with smaller papers first
4. Monitor API usage and costs
