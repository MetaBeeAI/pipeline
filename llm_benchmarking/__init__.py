"""
LLM Benchmarking Module

This module provides tools for generating test datasets and evaluating LLM outputs
using DeepEval for benchmarking purposes.

Main Components:
- Dataset Generation:
  - test_dataset_generation.py: Generates test datasets from reviewer answers
  - reviewer_dataset_generation.py: Generates reviewer comparison datasets
  - merge_answers.py: Merges LLM and reviewer answers for dataset creation

- DeepEval Evaluation:
  - deepeval_benchmarking.py: Traditional DeepEval metrics evaluation
  - deepeval_GEval.py: G-Eval correctness assessment
  - deepeval_reviewers.py: Reviewer comparison evaluation
  - deepeval_results_analysis.py: Results consolidation and visualization

- Analysis:
  - reviewer_rating.py: Analyzes reviewer ratings and generates statistical plots
"""

from .merge_answers import merge_answers
from .reviewer_rating import (
    calculate_question_stats,
    calculate_reviewer_agreement,
    calculate_reviewer_individual_stats,
    plot_question_ratings,
    plot_reviewer_agreement,
    plot_individual_reviewer_ratings
)

__version__ = "1.0.0"
__author__ = "MetaBeeAI Pipeline"

__all__ = [
    "merge_answers",
    "calculate_question_stats",
    "calculate_reviewer_agreement", 
    "calculate_reviewer_individual_stats",
    "plot_question_ratings",
    "plot_reviewer_agreement",
    "plot_individual_reviewer_ratings"
]
